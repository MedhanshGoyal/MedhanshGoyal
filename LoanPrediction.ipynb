{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7I9y9is21oS",
        "outputId": "e0822126-ddfb-4200-8cef-acd3f7eee6aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NW6lCHJBmmc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier,plot_tree as plot_decision_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_rows', 500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY9v0qm2DIz9"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Data/invoice_level_data_full_1.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuBPxNhBq9Mk",
        "outputId": "ce3c49d5-c2ca-4338-b279-7e5c2d3bb0d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "138177\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "df['disbursal_date'] = pd.to_datetime(df['disbursal_date'])\n",
        "target_date = datetime.datetime(2024, 2, 1)\n",
        "df = df[df['disbursal_date']>target_date]\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bboS3plk33u8"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "df['overdue_date'] = pd.to_datetime(df['overdue_date'])\n",
        "df['repaid_date'] = pd.to_datetime(df['repaid_date'])\n",
        "df['disbursal_date'] = pd.to_datetime(df['disbursal_date'])\n",
        "# Get today's date\n",
        "today = datetime.today()\n",
        "\n",
        "# Filter rows\n",
        "df = df[(df['repaid_date'].notnull()) | (df['overdue_date'] <= today)]\n",
        "\n",
        "df['dd_tenor'] = (df['overdue_date']-df['disbursal_date']).dt.days\n",
        "\n",
        "df['repaid_date'] = pd.to_datetime(df['repaid_date'])\n",
        "# Define a function to calculate the difference in days\n",
        "def calculate_days(row):\n",
        "    if pd.isnull(row['repaid_date']):\n",
        "        return 10000\n",
        "    else:\n",
        "        return (row['repaid_date'] - row['overdue_date']).days\n",
        "\n",
        "def determine_rpay(row):\n",
        "    if pd.isnull(row['repaid_date']):\n",
        "        return 120000\n",
        "    else:\n",
        "        return (row['repaid_date'] - row['disbursal_date']).days\n",
        "\n",
        "df['repayment_days'] = df.apply(determine_rpay,axis=1)\n",
        "df['cashback_date'] = pd.to_datetime(df['cashback_date'])\n",
        "\n",
        "# def determine_offer(row):\n",
        "#     if pd.isnull(row['repaid_date']):\n",
        "#         return 0\n",
        "#     elif row['cashback_date'] > row['repaid_date']:\n",
        "#         return 1\n",
        "#     else:\n",
        "#         return 0\n",
        "\n",
        "# df['offer'] = df.apply(determine_offer, axis=1)\n",
        "\n",
        "def determine_default(row):\n",
        "    if pd.isnull(row['repaid_date']):\n",
        "        return 0\n",
        "    elif row['overdue_date'] > row['repaid_date']:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "df['default'] = df.apply(determine_default, axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkItkRFxH4Fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O96ttdXMEiMc"
      },
      "outputs": [],
      "source": [
        "def clean_df(df):\n",
        "    columns_to_drop = [\n",
        "        'Unnamed: 0', 'facility_id', 'case_id', 'customer_id', 'buyer_id', 'buyer_name', 'buyer_gstin', 'seller_id',\n",
        "        'seller_name', 'seller_gstin', 'anchor_id', 'anchor_gstin', 'anchor_name', 'constitution', 'first_disbursal_date',\n",
        "        'is_term_loan', 'rff_id', 'invoice_no', 'invoice_date', 'cashback_date', 'overdue_date', 'repaid_date', 'dwd_date',\n",
        "        'pincode', 'asm_v2', 'zonal_head_v2', 'branch_head_v2', 'state_head_v2', 'progcap_arp_before_disbursal',\n",
        "        'cibil_score', 'cibil_pull_date', 'churn'\n",
        "    ]\n",
        "\n",
        "    # Find which columns are actually present in the DataFrame\n",
        "    columns_to_drop_present = [col for col in columns_to_drop if col in df.columns]\n",
        "\n",
        "    return df.drop(columns=columns_to_drop_present)\n",
        "\n",
        "df = clean_df(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PJ60CJYO_MNx"
      },
      "outputs": [],
      "source": [
        "# Select all columns except 'disbursal_date'\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns.difference(['disbursal_date']).tolist()\n",
        "\n",
        "# Initialize OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# One-hot encode the selected categorical columns\n",
        "one_hot_encoded = encoder.fit_transform(df[categorical_columns])\n",
        "\n",
        "# Create a DataFrame with the one-hot encoded features\n",
        "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Concatenate the original DataFrame with the one-hot encoded DataFrame\n",
        "df_encoded = pd.concat([df, one_hot_df], axis=1)\n",
        "\n",
        "# Drop the original categorical columns from the encoded DataFrame\n",
        "df_encoded = df_encoded.drop(categorical_columns, axis=1)\n",
        "\n",
        "# Update the original DataFrame with the encoded DataFrame\n",
        "df = df_encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATatyylgV9-w",
        "outputId": "e1b7e23e-4444-457e-cb72-88c7bd6bc504"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((83941, 572), (50721, 572))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import datetime\n",
        "df['disbursal_date'] = pd.to_datetime(df['disbursal_date'])\n",
        "target_date = datetime.datetime(2024, 3, 31)\n",
        "train = df[df['disbursal_date'] <= target_date]\n",
        "test = df[df['disbursal_date']>target_date]\n",
        "train = train.drop(columns = ['disbursal_date'],axis=1)\n",
        "test = test.drop(columns = ['disbursal_date'],axis=1)\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "train.shape,test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn050FHDIKqb"
      },
      "outputs": [],
      "source": [
        "train.dropna(inplace=True)\n",
        "test.dropna(inplace=True)\n",
        "X_train =train.drop('default', axis=1)  # Features\n",
        "y_train = train['default']  # Target variable\n",
        "\n",
        "# Instantiate ADASYN\n",
        "adasyn = ADASYN()\n",
        "\n",
        "# Apply ADASYN oversampling\n",
        "X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Concatenate X_resampled and y_resampled to get the oversampled dataset\n",
        "train = pd.concat([X_resampled, y_resampled], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03_s5zdyIeHT"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_test =test.drop('default', axis=1)  # Features\n",
        "y_test = test['default']  # Target variable\n",
        "\n",
        "# Instantiate ADASYN\n",
        "adasyn = ADASYN()\n",
        "\n",
        "# Apply ADASYN oversampling\n",
        "X_resampled1, y_resampled1 = adasyn.fit_resample(X_test, y_test)\n",
        "\n",
        "# Concatenate X_resampled and y_resampled to get the oversampled dataset\n",
        "test = pd.concat([X_resampled1, y_resampled1], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BprWMX6gWCE5"
      },
      "outputs": [],
      "source": [
        "def encode(df):\n",
        "    df_obj = df.select_dtypes(include=['object'])\n",
        "    encoders = {}\n",
        "    for col in df_obj.columns:\n",
        "        encoder = LabelEncoder()\n",
        "        df[col] = encoder.fit_transform(df[col])\n",
        "        encoders[col] = encoder\n",
        "    with open('LE_mdl_v1.pkl', 'wb') as f:\n",
        "        pickle.dump(encoders, f)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPSYvxNdSden",
        "outputId": "e88eb97c-6643-4e2f-97cc-7510f2800c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.4269319  -0.4184627   0.44781269 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]\n",
            " [-0.39833434 -0.38439276 -0.60597442 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]\n",
            " [ 0.53394602  0.72628723 -0.60597442 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]\n",
            " ...\n",
            " [ 1.18861557  1.50623326 -0.60597442 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]\n",
            " [ 0.76988611  1.00737642 -0.60597442 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]\n",
            " [-0.43904571 -0.43289458 -0.60597442 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]]\n"
          ]
        }
      ],
      "source": [
        "train.dropna(inplace=True)\n",
        "test.dropna(inplace=True)\n",
        "scaler = StandardScaler()\n",
        "# Separate features and target variables for train and test datasets\n",
        "x_train = train.drop(columns=[ 'repayment_days','default'],axis=1)\n",
        "y_train = train['repayment_days']\n",
        "Y_train = train['default']\n",
        "\n",
        "x_val = test.drop(columns=[ 'repayment_days','default'])\n",
        "y_val = test['repayment_days']\n",
        "Y_val = test['default']\n",
        "\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_val_scaled = scaler.transform(x_val)\n",
        "print(x_val_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MkSBkG0SbUO"
      },
      "outputs": [],
      "source": [
        "model1 = LogisticRegression()\n",
        "model2 = RandomForestClassifier()\n",
        "model3 = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWkP7yaWWVAs"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "def model_train(model, x_train, y_train, x_test, y_test,x1):\n",
        "    # Ensure y_test is 1-dimensional\n",
        "    y_test = np.ravel(y_test)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    # Ensure y_pred is 1-dimensional\n",
        "    y_pred = np.ravel(y_pred)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    print('Accuracy Score:', accuracy_score(y_test, y_pred))\n",
        "\n",
        "    # Print classification report\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        feature_importances = pd.Series(model.feature_importances_, index=x1.columns)\n",
        "        top_feature_importances = feature_importances\n",
        "        feature_importances.plot(kind='barh')\n",
        "        plt.title('Top 10 Feature Importances')\n",
        "        plt.xlabel('Relative Importance')\n",
        "        plt.ylabel('Features')\n",
        "        plt.show()\n",
        "    top_feature_importances.to_csv('feature_importances.csv')\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    unique_labels = np.unique(np.concatenate((y_test, y_pred)))\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred, labels=unique_labels)\n",
        "\n",
        "    # Convert confusion matrix to DataFrame\n",
        "    conf_matrix_df = pd.DataFrame(conf_matrix,\n",
        "                                  index=unique_labels,\n",
        "                                  columns=unique_labels)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    conf_matrix_df.to_csv('confusion_matrix.csv')\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print('Confusion Matrix:')\n",
        "    print(conf_matrix_df)\n",
        "\n",
        "    # Save the trained model\n",
        "    with open(str(model.__class__.__name__) + '_mdl.pkl', 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # # Save y_pred and y_test to the same CSV file side by side\n",
        "    # x_test['y_test'] = y_test\n",
        "    # x_test['y_pred'] = y_pred\n",
        "\n",
        "    # # Save x_test, y_test, and y_pred to the same CSV file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4TrDZRlgDGa"
      },
      "outputs": [],
      "source": [
        "model_train(model3, x_train, Y_train, x_val, Y_val,x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v_F0r49kzcO"
      },
      "outputs": [],
      "source": [
        "# model3.fit(x_train_scaled, y_train)\n",
        "\n",
        "#     # Make predictions\n",
        "# y_pred = model3.predict(x_val_scaled)\n",
        "# x_val['y_test'] = y_val\n",
        "# x_val['y_pred'] = y_pred\n",
        "\n",
        "#     # # Save x_test, y_test, and y_pred to the same CSV file\n",
        "# x_val.to_csv('x1_y_pred_y_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2grZ9sdtTYH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = SVC(kernel='linear')  # You can choose different kernels like 'rbf', 'poly', etc.\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model.fit(x_train_scaled, Y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "predictions = svm_model.predict(x_val_scaled)\n",
        "\n",
        "# Evaluate the model (you might want to use different metrics based on your problem)\n",
        "accuracy = svm_model.score(x_val_scaled, Y_val)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsf5EySL7kH2"
      },
      "outputs": [],
      "source": [
        "coef_matrix = pd.DataFrame(svm_model.coef_.T, index=x_train.columns)\n",
        "\n",
        "# Save the coefficient matrix to a CSV file\n",
        "coef_matrix.to_csv('svm_coefficients.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JppEJ1t_B0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model1 = SVC(kernel='linear')  # You can choose different kernels like 'rbf', 'poly', etc.\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model1.fit(x_train_scaled, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "predictions = svm_model1.predict(x_val_scaled)\n",
        "\n",
        "# Evaluate the model (you might want to use different metrics based on your problem)\n",
        "accuracy = svm_model.score(x_val_scaled, y_val)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FEq6bvryyPR"
      },
      "outputs": [],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG2HsLqnw-2I"
      },
      "outputs": [],
      "source": [
        "feature_weights = pd.DataFrame({'Feature': x_train.columns, 'Weight': svm_model1.coef_[0]})\n",
        "print(\"Feature Weights:\")\n",
        "print(feature_weights)\n",
        "\n",
        "# Plotting decision boundary (works only for 2D or 3D data)\n",
        "if len(x_train.columns) <= 3:\n",
        "    if len(x_train.columns) == 2:\n",
        "        # For 2D data\n",
        "        x_min, x_max = x_train_scaled[:, 0].min() - 1, x_train_scaled[:, 0].max() + 1\n",
        "        y_min, y_max = x_train_scaled[:, 1].min() - 1, x_train_scaled[:, 1].max() + 1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                             np.arange(y_min, y_max, 0.02))\n",
        "        Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "        plt.scatter(x_train_scaled[:, 0], x_train_scaled[:, 1], c=Y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
        "        plt.xlabel('Feature 1')\n",
        "        plt.ylabel('Feature 2')\n",
        "        plt.title('Decision Boundary')\n",
        "        plt.show()\n",
        "    elif len(x_train.columns) == 3:\n",
        "        # For 3D data\n",
        "        from mpl_toolkits.mplot3d import Axes3D\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        xx, yy = np.meshgrid(np.linspace(x_train_scaled[:, 0].min(), x_train_scaled[:, 0].max(), 50),\n",
        "                             np.linspace(x_train_scaled[:, 1].min(), x_train_scaled[:, 1].max(), 50))\n",
        "        Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        ax.contour3D(xx, yy, Z, 50, cmap='coolwarm')\n",
        "        ax.scatter(x_train_scaled[:, 0], x_train_scaled[:, 1], c=Y_train, cmap='coolwarm', s=20, edgecolors='k')\n",
        "        ax.set_xlabel('Feature 1')\n",
        "        ax.set_ylabel('Feature 2')\n",
        "        ax.set_zlabel('Feature 3')\n",
        "        ax.set_title('Decision Boundary')\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Cannot plot decision boundary for data with more than 3 features.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
