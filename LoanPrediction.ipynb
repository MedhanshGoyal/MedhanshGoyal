{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7I9y9is21oS",
        "outputId": "e0822126-ddfb-4200-8cef-acd3f7eee6aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NW6lCHJBmmc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier,plot_tree as plot_decision_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_rows', 500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY9v0qm2DIz9"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Data/invoice_level_data_full_1.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuBPxNhBq9Mk",
        "outputId": "ce3c49d5-c2ca-4338-b279-7e5c2d3bb0d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "138177\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "df['disbursal_date'] = pd.to_datetime(df['disbursal_date'])\n",
        "target_date = datetime.datetime(2024, 2, 1)\n",
        "df = df[df['disbursal_date']>target_date]\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bboS3plk33u8"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "df['overdue_date'] = pd.to_datetime(df['overdue_date'])\n",
        "df['repaid_date'] = pd.to_datetime(df['repaid_date'])\n",
        "df['disbursal_date'] = pd.to_datetime(df['disbursal_date'])\n",
        "# Get today's date\n",
        "today = datetime.today()\n",
        "\n",
        "# Filter rows\n",
        "df = df[(df['repaid_date'].notnull()) | (df['overdue_date'] <= today)]\n",
        "\n",
        "df['dd_tenor'] = (df['overdue_date']-df['disbursal_date']).dt.days\n",
        "\n",
        "df['repaid_date'] = pd.to_datetime(df['repaid_date'])\n",
        "# Define a function to calculate the difference in days\n",
        "def calculate_days(row):\n",
        "    if pd.isnull(row['repaid_date']):\n",
        "        return 10000\n",
        "    else:\n",
        "        return (row['repaid_date'] - row['overdue_date']).days\n",
        "\n",
        "def determine_rpay(row):\n",
        "    if pd.isnull(row['repaid_date']):\n",
        "        return 120000\n",
        "    else:\n",
        "        return (row['repaid_date'] - row['disbursal_date']).days\n",
        "\n",
        "df['repayment_days'] = df.apply(determine_rpay,axis=1)\n",
        "df['cashback_date'] = pd.to_datetime(df['cashback_date'])\n",
        "\n",
        "# def determine_offer(row):\n",
        "#     if pd.isnull(row['repaid_date']):\n",
        "#         return 0\n",
        "#     elif row['cashback_date'] > row['repaid_date']:\n",
        "#         return 1\n",
        "#     else:\n",
        "#         return 0\n",
        "\n",
        "# df['offer'] = df.apply(determine_offer, axis=1)\n",
        "\n",
        "def determine_default(row):\n",
        "    if pd.isnull(row['repaid_date']):\n",
        "        return 0\n",
        "    elif row['overdue_date'] > row['repaid_date']:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "df['default'] = df.apply(determine_default, axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkItkRFxH4Fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vunY9Om-qau",
        "outputId": "0711f222-5459-468c-9876-2e50ef06e7b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 138177 entries, 3 to 1519857\n",
            "Data columns (total 53 columns):\n",
            " #   Column                                Non-Null Count   Dtype         \n",
            "---  ------                                --------------   -----         \n",
            " 0   Unnamed: 0                            138177 non-null  int64         \n",
            " 1   facility_id                           138177 non-null  int64         \n",
            " 2   case_id                               138177 non-null  float64       \n",
            " 3   customer_id                           138177 non-null  object        \n",
            " 4   buyer_id                              138177 non-null  float64       \n",
            " 5   buyer_name                            138177 non-null  object        \n",
            " 6   buyer_gstin                           138177 non-null  object        \n",
            " 7   seller_id                             138177 non-null  float64       \n",
            " 8   seller_name                           138177 non-null  object        \n",
            " 9   seller_gstin                          138177 non-null  object        \n",
            " 10  anchor_id                             138177 non-null  float64       \n",
            " 11  anchor_gstin                          138177 non-null  object        \n",
            " 12  anchor_name                           138177 non-null  object        \n",
            " 13  industry_name                         138177 non-null  object        \n",
            " 14  program_type                          138177 non-null  object        \n",
            " 15  buyer_city                            137267 non-null  object        \n",
            " 16  state_name                            138177 non-null  object        \n",
            " 17  constitution                          138177 non-null  object        \n",
            " 18  first_disbursal_date                  138177 non-null  object        \n",
            " 19  is_term_loan                          138177 non-null  int64         \n",
            " 20  rff_id                                138177 non-null  int64         \n",
            " 21  invoice_no                            138177 non-null  object        \n",
            " 22  invoice_date                          138177 non-null  object        \n",
            " 23  invoice_amount                        138177 non-null  float64       \n",
            " 24  disbursal_date                        138177 non-null  datetime64[ns]\n",
            " 25  principal_amount                      138177 non-null  float64       \n",
            " 26  cashback_date                         138177 non-null  datetime64[ns]\n",
            " 27  overdue_date                          138177 non-null  datetime64[ns]\n",
            " 28  repaid_date                           136270 non-null  datetime64[ns]\n",
            " 29  dwd_date                              138177 non-null  object        \n",
            " 30  total_associates                      138151 non-null  float64       \n",
            " 31  female_associates                     138151 non-null  float64       \n",
            " 32  max_associate_age                     138047 non-null  float64       \n",
            " 33  min_associate_age                     138047 non-null  float64       \n",
            " 34  pincode                               136402 non-null  object        \n",
            " 35  asm_v2                                138177 non-null  object        \n",
            " 36  zonal_head_v2                         138177 non-null  object        \n",
            " 37  branch_head_v2                        138177 non-null  object        \n",
            " 38  state_head_v2                         138177 non-null  object        \n",
            " 39  total_invoices_disbursed_all_time     138177 non-null  int64         \n",
            " 40  non_overdue_invoices_all_time         138177 non-null  int64         \n",
            " 41  overdue_invoices_all_time             138177 non-null  int64         \n",
            " 42  progcap_arp_before_disbursal          136123 non-null  object        \n",
            " 43  count_open_invoices_before_disbursal  138177 non-null  int64         \n",
            " 44  utilized_limit                        138157 non-null  float64       \n",
            " 45  repayment_amount                      135379 non-null  float64       \n",
            " 46  sub_limit                             138155 non-null  float64       \n",
            " 47  cibil_score                           0 non-null       float64       \n",
            " 48  cibil_pull_date                       0 non-null       object        \n",
            " 49  churn                                 135403 non-null  float64       \n",
            " 50  dd_tenor                              138177 non-null  int64         \n",
            " 51  repayment_days                        138177 non-null  int64         \n",
            " 52  default                               138177 non-null  int64         \n",
            "dtypes: datetime64[ns](4), float64(15), int64(11), object(23)\n",
            "memory usage: 56.9+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O96ttdXMEiMc"
      },
      "outputs": [],
      "source": [
        "def clean_df(df):\n",
        "    columns_to_drop = [\n",
        "        'Unnamed: 0', 'facility_id', 'case_id', 'customer_id', 'buyer_id', 'buyer_name', 'buyer_gstin', 'seller_id',\n",
        "        'seller_name', 'seller_gstin', 'anchor_id', 'anchor_gstin', 'anchor_name', 'constitution', 'first_disbursal_date',\n",
        "        'is_term_loan', 'rff_id', 'invoice_no', 'invoice_date', 'cashback_date', 'overdue_date', 'repaid_date', 'dwd_date',\n",
        "        'pincode', 'asm_v2', 'zonal_head_v2', 'branch_head_v2', 'state_head_v2', 'progcap_arp_before_disbursal',\n",
        "        'cibil_score', 'cibil_pull_date', 'churn'\n",
        "    ]\n",
        "\n",
        "    # Find which columns are actually present in the DataFrame\n",
        "    columns_to_drop_present = [col for col in columns_to_drop if col in df.columns]\n",
        "\n",
        "    return df.drop(columns=columns_to_drop_present)\n",
        "\n",
        "df = clean_df(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dejOhAHAV3Y-",
        "outputId": "6cb293ab-82d3-4cd8-d93f-afecc27a44bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 138177 entries, 3 to 1519857\n",
            "Data columns (total 21 columns):\n",
            " #   Column                                Non-Null Count   Dtype         \n",
            "---  ------                                --------------   -----         \n",
            " 0   industry_name                         138177 non-null  object        \n",
            " 1   program_type                          138177 non-null  object        \n",
            " 2   buyer_city                            137267 non-null  object        \n",
            " 3   state_name                            138177 non-null  object        \n",
            " 4   invoice_amount                        138177 non-null  float64       \n",
            " 5   disbursal_date                        138177 non-null  datetime64[ns]\n",
            " 6   principal_amount                      138177 non-null  float64       \n",
            " 7   total_associates                      138151 non-null  float64       \n",
            " 8   female_associates                     138151 non-null  float64       \n",
            " 9   max_associate_age                     138047 non-null  float64       \n",
            " 10  min_associate_age                     138047 non-null  float64       \n",
            " 11  total_invoices_disbursed_all_time     138177 non-null  int64         \n",
            " 12  non_overdue_invoices_all_time         138177 non-null  int64         \n",
            " 13  overdue_invoices_all_time             138177 non-null  int64         \n",
            " 14  count_open_invoices_before_disbursal  138177 non-null  int64         \n",
            " 15  utilized_limit                        138157 non-null  float64       \n",
            " 16  repayment_amount                      135379 non-null  float64       \n",
            " 17  sub_limit                             138155 non-null  float64       \n",
            " 18  dd_tenor                              138177 non-null  int64         \n",
            " 19  repayment_days                        138177 non-null  int64         \n",
            " 20  default                               138177 non-null  int64         \n",
            "dtypes: datetime64[ns](1), float64(9), int64(7), object(4)\n",
            "memory usage: 23.2+ MB\n"
          ]
        }
      ],
      "source": [
        "df['dd_tenor'] = pd.to_numeric(df['dd_tenor'], errors='coerce')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-theKGxUCMAM",
        "outputId": "81637a40-ebdf-48fe-883a-15d502b697b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 134662 entries, 3 to 1519857\n",
            "Data columns (total 21 columns):\n",
            " #   Column                                Non-Null Count   Dtype         \n",
            "---  ------                                --------------   -----         \n",
            " 0   industry_name                         134662 non-null  object        \n",
            " 1   program_type                          134662 non-null  object        \n",
            " 2   buyer_city                            134662 non-null  object        \n",
            " 3   state_name                            134662 non-null  object        \n",
            " 4   invoice_amount                        134662 non-null  float64       \n",
            " 5   disbursal_date                        134662 non-null  datetime64[ns]\n",
            " 6   principal_amount                      134662 non-null  float64       \n",
            " 7   total_associates                      134662 non-null  float64       \n",
            " 8   female_associates                     134662 non-null  float64       \n",
            " 9   max_associate_age                     134662 non-null  float64       \n",
            " 10  min_associate_age                     134662 non-null  float64       \n",
            " 11  total_invoices_disbursed_all_time     134662 non-null  int64         \n",
            " 12  non_overdue_invoices_all_time         134662 non-null  int64         \n",
            " 13  overdue_invoices_all_time             134662 non-null  int64         \n",
            " 14  count_open_invoices_before_disbursal  134662 non-null  int64         \n",
            " 15  utilized_limit                        134662 non-null  float64       \n",
            " 16  repayment_amount                      134662 non-null  float64       \n",
            " 17  sub_limit                             134662 non-null  float64       \n",
            " 18  dd_tenor                              134662 non-null  int64         \n",
            " 19  repayment_days                        134662 non-null  int64         \n",
            " 20  default                               134662 non-null  int64         \n",
            "dtypes: datetime64[ns](1), float64(9), int64(7), object(4)\n",
            "memory usage: 22.6+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "df=df.dropna()\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PJ60CJYO_MNx"
      },
      "outputs": [],
      "source": [
        "# Select all columns except 'disbursal_date'\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns.difference(['disbursal_date']).tolist()\n",
        "\n",
        "# Initialize OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# One-hot encode the selected categorical columns\n",
        "one_hot_encoded = encoder.fit_transform(df[categorical_columns])\n",
        "\n",
        "# Create a DataFrame with the one-hot encoded features\n",
        "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Concatenate the original DataFrame with the one-hot encoded DataFrame\n",
        "df_encoded = pd.concat([df, one_hot_df], axis=1)\n",
        "\n",
        "# Drop the original categorical columns from the encoded DataFrame\n",
        "df_encoded = df_encoded.drop(categorical_columns, axis=1)\n",
        "\n",
        "# Update the original DataFrame with the encoded DataFrame\n",
        "df = df_encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATatyylgV9-w",
        "outputId": "e1b7e23e-4444-457e-cb72-88c7bd6bc504"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((83941, 572), (50721, 572))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import datetime\n",
        "df['disbursal_date'] = pd.to_datetime(df['disbursal_date'])\n",
        "target_date = datetime.datetime(2024, 3, 31)\n",
        "train = df[df['disbursal_date'] <= target_date]\n",
        "test = df[df['disbursal_date']>target_date]\n",
        "train = train.drop(columns = ['disbursal_date'],axis=1)\n",
        "test = test.drop(columns = ['disbursal_date'],axis=1)\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "train.shape,test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn050FHDIKqb"
      },
      "outputs": [],
      "source": [
        "train.dropna(inplace=True)\n",
        "test.dropna(inplace=True)\n",
        "X_train =train.drop('default', axis=1)  # Features\n",
        "y_train = train['default']  # Target variable\n",
        "\n",
        "# Instantiate ADASYN\n",
        "adasyn = ADASYN()\n",
        "\n",
        "# Apply ADASYN oversampling\n",
        "X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Concatenate X_resampled and y_resampled to get the oversampled dataset\n",
        "train = pd.concat([X_resampled, y_resampled], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03_s5zdyIeHT"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_test =test.drop('default', axis=1)  # Features\n",
        "y_test = test['default']  # Target variable\n",
        "\n",
        "# Instantiate ADASYN\n",
        "adasyn = ADASYN()\n",
        "\n",
        "# Apply ADASYN oversampling\n",
        "X_resampled1, y_resampled1 = adasyn.fit_resample(X_test, y_test)\n",
        "\n",
        "# Concatenate X_resampled and y_resampled to get the oversampled dataset\n",
        "test = pd.concat([X_resampled1, y_resampled1], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BprWMX6gWCE5"
      },
      "outputs": [],
      "source": [
        "def encode(df):\n",
        "    df_obj = df.select_dtypes(include=['object'])\n",
        "    encoders = {}\n",
        "    for col in df_obj.columns:\n",
        "        encoder = LabelEncoder()\n",
        "        df[col] = encoder.fit_transform(df[col])\n",
        "        encoders[col] = encoder\n",
        "    with open('LE_mdl_v1.pkl', 'wb') as f:\n",
        "        pickle.dump(encoders, f)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPSYvxNdSden",
        "outputId": "e88eb97c-6643-4e2f-97cc-7510f2800c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.4269319  -0.4184627   0.44781269 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]\n",
            " [-0.39833434 -0.38439276 -0.60597442 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]\n",
            " [ 0.53394602  0.72628723 -0.60597442 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]\n",
            " ...\n",
            " [ 1.18861557  1.50623326 -0.60597442 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]\n",
            " [ 0.76988611  1.00737642 -0.60597442 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]\n",
            " [-0.43904571 -0.43289458 -0.60597442 ... -0.48547696 -0.08374497\n",
            "  -0.25556452]]\n"
          ]
        }
      ],
      "source": [
        "train.dropna(inplace=True)\n",
        "test.dropna(inplace=True)\n",
        "scaler = StandardScaler()\n",
        "# Separate features and target variables for train and test datasets\n",
        "x_train = train.drop(columns=[ 'repayment_days','default'],axis=1)\n",
        "y_train = train['repayment_days']\n",
        "Y_train = train['default']\n",
        "\n",
        "x_val = test.drop(columns=[ 'repayment_days','default'])\n",
        "y_val = test['repayment_days']\n",
        "Y_val = test['default']\n",
        "\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_val_scaled = scaler.transform(x_val)\n",
        "print(x_val_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MkSBkG0SbUO"
      },
      "outputs": [],
      "source": [
        "model1 = LogisticRegression()\n",
        "model2 = RandomForestClassifier()\n",
        "model3 = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWkP7yaWWVAs"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "def model_train(model, x_train, y_train, x_test, y_test,x1):\n",
        "    # Ensure y_test is 1-dimensional\n",
        "    y_test = np.ravel(y_test)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    # Ensure y_pred is 1-dimensional\n",
        "    y_pred = np.ravel(y_pred)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    print('Accuracy Score:', accuracy_score(y_test, y_pred))\n",
        "\n",
        "    # Print classification report\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        feature_importances = pd.Series(model.feature_importances_, index=x1.columns)\n",
        "        top_feature_importances = feature_importances\n",
        "        feature_importances.plot(kind='barh')\n",
        "        plt.title('Top 10 Feature Importances')\n",
        "        plt.xlabel('Relative Importance')\n",
        "        plt.ylabel('Features')\n",
        "        plt.show()\n",
        "    top_feature_importances.to_csv('feature_importances.csv')\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    unique_labels = np.unique(np.concatenate((y_test, y_pred)))\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred, labels=unique_labels)\n",
        "\n",
        "    # Convert confusion matrix to DataFrame\n",
        "    conf_matrix_df = pd.DataFrame(conf_matrix,\n",
        "                                  index=unique_labels,\n",
        "                                  columns=unique_labels)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    conf_matrix_df.to_csv('confusion_matrix.csv')\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print('Confusion Matrix:')\n",
        "    print(conf_matrix_df)\n",
        "\n",
        "    # Save the trained model\n",
        "    with open(str(model.__class__.__name__) + '_mdl.pkl', 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # # Save y_pred and y_test to the same CSV file side by side\n",
        "    # x_test['y_test'] = y_test\n",
        "    # x_test['y_pred'] = y_pred\n",
        "\n",
        "    # # Save x_test, y_test, and y_pred to the same CSV file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "GirsQPamW9HQ",
        "outputId": "b9caaba6-1f17-44a4-8a1e-701d395b6d69"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Unknown label type: 'continuous'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9368c50e4f72>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-81f0f408c505>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(model, x_train, y_train, x_test, y_test, x1)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_class_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_y_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mDOUBLE\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_y_class_weight\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_y_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m\"multilabel-sequences\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     ]:\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
          ]
        }
      ],
      "source": [
        "model_train(model2, x_train_scaled, y_train, x_val_scaled, y_val,x_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4TrDZRlgDGa"
      },
      "outputs": [],
      "source": [
        "model_train(model3, x_train, Y_train, x_val, Y_val,x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v_F0r49kzcO"
      },
      "outputs": [],
      "source": [
        "# model3.fit(x_train_scaled, y_train)\n",
        "\n",
        "#     # Make predictions\n",
        "# y_pred = model3.predict(x_val_scaled)\n",
        "# x_val['y_test'] = y_val\n",
        "# x_val['y_pred'] = y_pred\n",
        "\n",
        "#     # # Save x_test, y_test, and y_pred to the same CSV file\n",
        "# x_val.to_csv('x1_y_pred_y_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2grZ9sdtTYH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = SVC(kernel='linear')  # You can choose different kernels like 'rbf', 'poly', etc.\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model.fit(x_train_scaled, Y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "predictions = svm_model.predict(x_val_scaled)\n",
        "\n",
        "# Evaluate the model (you might want to use different metrics based on your problem)\n",
        "accuracy = svm_model.score(x_val_scaled, Y_val)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsf5EySL7kH2"
      },
      "outputs": [],
      "source": [
        "coef_matrix = pd.DataFrame(svm_model.coef_.T, index=x_train.columns)\n",
        "\n",
        "# Save the coefficient matrix to a CSV file\n",
        "coef_matrix.to_csv('svm_coefficients.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JppEJ1t_B0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model1 = SVC(kernel='linear')  # You can choose different kernels like 'rbf', 'poly', etc.\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model1.fit(x_train_scaled, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "predictions = svm_model1.predict(x_val_scaled)\n",
        "\n",
        "# Evaluate the model (you might want to use different metrics based on your problem)\n",
        "accuracy = svm_model.score(x_val_scaled, y_val)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FEq6bvryyPR"
      },
      "outputs": [],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG2HsLqnw-2I"
      },
      "outputs": [],
      "source": [
        "feature_weights = pd.DataFrame({'Feature': x_train.columns, 'Weight': svm_model1.coef_[0]})\n",
        "print(\"Feature Weights:\")\n",
        "print(feature_weights)\n",
        "\n",
        "# Plotting decision boundary (works only for 2D or 3D data)\n",
        "if len(x_train.columns) <= 3:\n",
        "    if len(x_train.columns) == 2:\n",
        "        # For 2D data\n",
        "        x_min, x_max = x_train_scaled[:, 0].min() - 1, x_train_scaled[:, 0].max() + 1\n",
        "        y_min, y_max = x_train_scaled[:, 1].min() - 1, x_train_scaled[:, 1].max() + 1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                             np.arange(y_min, y_max, 0.02))\n",
        "        Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "        plt.scatter(x_train_scaled[:, 0], x_train_scaled[:, 1], c=Y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
        "        plt.xlabel('Feature 1')\n",
        "        plt.ylabel('Feature 2')\n",
        "        plt.title('Decision Boundary')\n",
        "        plt.show()\n",
        "    elif len(x_train.columns) == 3:\n",
        "        # For 3D data\n",
        "        from mpl_toolkits.mplot3d import Axes3D\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        xx, yy = np.meshgrid(np.linspace(x_train_scaled[:, 0].min(), x_train_scaled[:, 0].max(), 50),\n",
        "                             np.linspace(x_train_scaled[:, 1].min(), x_train_scaled[:, 1].max(), 50))\n",
        "        Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        ax.contour3D(xx, yy, Z, 50, cmap='coolwarm')\n",
        "        ax.scatter(x_train_scaled[:, 0], x_train_scaled[:, 1], c=Y_train, cmap='coolwarm', s=20, edgecolors='k')\n",
        "        ax.set_xlabel('Feature 1')\n",
        "        ax.set_ylabel('Feature 2')\n",
        "        ax.set_zlabel('Feature 3')\n",
        "        ax.set_title('Decision Boundary')\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Cannot plot decision boundary for data with more than 3 features.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}